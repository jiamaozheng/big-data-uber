#!/bin/bash

# Step 2.1
# Make sure we have already used Eclipse to Maven install the uberDataIngest project,
# and thus generated the jar file uber-uberDataIngest-0.0.1-SNAPSHOT.jar
# Then we set the variables of jar name and class name as follows
jarname="uber-uberDataIngest-0.0.1-SNAPSHOT.jar"
classname="edu.uchicago.mpcs53013.uberDataIngest.UberDataIngestTool"

# Step 2.2
# Specify the input and output directory paths
input="/home/mpcs53013/Desktop/uber_raw_data"    # Local file system, this is the directory generated by step 1
output="/xinran/input-uber-data"    # HDFS

# Step 2.3
# Clean up existing files in HDFS, if any
hdfs dfs -rm -R "$output"

# Step 2.4
# Ingest uber data of the year 2014
yarn jar "$jarname" "$classname" "$input" 2014 "$output"

# Step 2.5
# Ingest uber data of the year 2015
yarn jar "$jarname" "$classname" "$input" 2015 "$output"

# Step 2.6
# Copy the location ID lookup table .csv file for the year 2015 into HDFS
hdfs dfs -put "${input}/uber2015_lookup.csv" "$output"

# Step 2.7
# Check if all the uber related data have been ingested into HDFS as expected
hdfs dfs -ls -R "$output"


# The result should look like
# ......  352631213 ....../uber2014
# ...... 1039982166 ....../uber2015
# ......       7690 ....../uber2015_lookup.csv


